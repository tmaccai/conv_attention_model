Q:中文分词和embedding 

# 统计词语频数，去除低频词
# 先将句子进行word embedding后，传入LSTM序列进行训练，将LSTM的最后一个hidden state拿出来，加入全连接层得到最终输出结果。

# LSTM不需要对词向量求和，而是直接对词向量本身进行学习。其中无论是求和还是求平均，这种聚合性操作都会损失一定的信息
在model中，我们首先构造了LSTM单元，并且为了防止过拟合，添加了dropout；执行dynamic_rnn以后，我们会得到lstm最后的state，这是一个tuple结构，包含了cell state和hidden state（经过output gate的结果），我们这里只取hidden state输出，即lstm_state.h，对这个向量进行连接，最终得到输出结果

# jieba 分词偶尔有缺陷

# Adagrad
前期g_t较小的时候， regularizer较大，能够放大梯度
后期g_t较大的时候，regularizer较小，能够约束梯度

# Adadelta
Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。
Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。






Q: 用word2vec训练,test出现未知词怎么办
Q: lstm number of neurons in hidden layer
